import logging
import os
import sqlite3
import time

import pandas as pd
import requests

# ç¡®ä¿å¼•å…¥ä½ çš„æ—¶åŒºé…ç½®
try:
    from config.loader import TIMEZONE
except ImportError:
    TIMEZONE = "+8"  # å…œåº•é»˜è®¤å€¼

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class OKXDataLoader:
    def __init__(self, symbol="ETH-USDT-SWAP", timeframe="1H", db_dir=None):
        self.symbol = symbol
        self.timeframe = timeframe
        self.base_url = "https://www.okx.com"

        self.session = requests.Session()

        self.bar_map = {
            '1m': '1m',
            '5m': '5m',
            '15m': '15m',
            '30m': '30m',
            '1H': '1H',
            '4H': '4H',
            '1D': '1D'
        }
        if timeframe not in self.bar_map:
            raise IndexError(f"æ²¡æœ‰è¿™ä¸ªtimeframe: {timeframe}")
        self.okx_bar = self.bar_map.get(timeframe)

        # ==========================================
        # æ ¸å¿ƒä¿®æ”¹ï¼šåˆ©ç”¨ __file__ åŠ¨æ€è·å–é¡¹ç›®æ ¹ç›®å½•
        # ==========================================
        if db_dir is None:
            # è·å– okx_loader.py çš„ç»å¯¹è·¯å¾„
            current_file = os.path.abspath(__file__)
            # å‘ä¸Šæ¨ä¸‰å±‚ï¼šokx_loader.py -> data_feed -> src -> æ ¹ç›®å½• (Momentum1.66)
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_file)))
            # å¼ºè¡ŒæŠŠæ•°æ®åº“ç›®å½•é”å®šåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ data æ–‡ä»¶å¤¹é‡Œ
            db_dir = os.path.join(project_root, 'data')

        if not os.path.exists(db_dir):
            os.makedirs(db_dir)

        self.db_path = os.path.join(db_dir, 'crypto_history.db')
        self.table_name = f"{symbol.replace('-', '_')}_{timeframe}"

    def _get_db_connection(self):
        return sqlite3.connect(self.db_path)

    def _get_current_local_time(self):
        """è·å–å¸¦æœ‰é…ç½®æ—¶åŒºåç§»çš„å½“å‰æ—¶é—´"""
        now_utc = pd.Timestamp.utcnow().tz_localize(None)
        if "+" in TIMEZONE:
            now_utc += pd.Timedelta(hours=int(TIMEZONE.split("+")[-1]))
        elif "-" in TIMEZONE:
            now_utc -= pd.Timedelta(hours=int(TIMEZONE.split("-")[-1]))
        return now_utc

    def load_local_data(self) -> pd.DataFrame:
        try:
            conn = self._get_db_connection()
            cursor = conn.cursor()
            cursor.execute(f"SELECT count(name) FROM sqlite_master WHERE type='table' AND name='{self.table_name}'")
            if cursor.fetchone()[0] == 0:
                conn.close()
                return pd.DataFrame()

            df = pd.read_sql(f"SELECT * FROM {self.table_name}", conn, index_col='timestamp', parse_dates=['timestamp'])
            conn.close()
            return df
        except Exception as e:
            logging.error(f"è¯»å–æœ¬åœ°æ•°æ®åº“å¤±è´¥: {e}")
            return pd.DataFrame()

    def save_local_data(self, df: pd.DataFrame):
        if df.empty:
            return
        conn = self._get_db_connection()
        df.to_sql(self.table_name, conn, if_exists='replace', index=True)
        conn.close()
        logging.info(f"ğŸ’¾ æˆåŠŸå°† {len(df)} æ ¹ K çº¿ä¿å­˜è‡³æœ¬åœ°æ•°æ®åº“: [{self.table_name}]")

    def fetch_from_okx(self, limit=100, after_ts=None, max_retries=10) -> pd.DataFrame:
        """åŸç”Ÿè°ƒç”¨ OKX V5 æ¥å£æ‹‰å–å†å² K çº¿ (è‡ªåŠ¨åˆ†æ‰¹é˜²å°ç‰ˆ)"""
        endpoint = "/api/v5/market/history-candles"
        url = f"{self.base_url}{endpoint}"

        all_candles = []
        current_after = after_ts
        batch_size_threshold = 1000

        # logging.info(f"å¼€å§‹é€šè¿‡åŸç”Ÿ API æ‰¹é‡æ‹‰å– {self.symbol} {self.timeframe} æ•°æ®ï¼Œç›®æ ‡ {limit} æ ¹...")

        while len(all_candles) < limit:
            fetch_size = min(100, limit - len(all_candles))
            params = {
                "instId": self.symbol,
                "bar": self.okx_bar,
                "limit": fetch_size
            }
            if current_after:
                params["after"] = current_after

            candles = []
            success = False

            for attempt in range(max_retries):
                try:
                    response = self.session.get(url, params=params, timeout=15)
                    response.raise_for_status()
                    data = response.json()

                    if data.get("code") != "0":
                        raise ValueError(f"OKX ä¸šåŠ¡æŠ¥é”™: {data.get('msg')}")

                    candles = data.get("data", [])
                    if not candles:
                        success = True
                        break

                    all_candles.extend(candles)
                    current_after = candles[-1][0]
                    success = True

                    if len(all_candles) % 10000 == 0 or len(all_candles) == limit:
                        logging.info(f"æ‹‰å–è¿›åº¦: {len(all_candles)} / {limit} ...")

                    break

                except Exception as e:
                    logging.warning(
                        f"ç½‘ç»œé¢ ç°¸ (è¿›åº¦ {len(all_candles)}/{limit}) | ç¬¬ {attempt + 1}/{max_retries} æ¬¡é‡è¯•... æŠ¥é”™: {e}")
                    self.session.close()
                    self.session = requests.Session()
                    sleep_time = 3 + (attempt * 2)
                    time.sleep(sleep_time)

            if not success or not candles:
                logging.error(f"ä¸¥é‡ç½‘ç»œæ•…éšœæˆ–æ— æ›´å¤šæ•°æ®ã€‚åœæ­¢æ‹‰å–ï¼å°†è¿”å›å·²æˆåŠŸè·å–çš„ {len(all_candles)} æ ¹æ•°æ®ã€‚")
                break

            if len(all_candles) > 0 and len(all_candles) % batch_size_threshold == 0:
                # logging.debug(f"ğŸŸ¢ å·²å®Œæˆä¸€ä¸ªå¤§æ‰¹æ¬¡ ({len(all_candles)}æ ¹)ï¼Œå¼ºåˆ¶ä¼‘çœ  3 ç§’ï¼Œé˜²å°é”...")
                time.sleep(3)
            else:
                time.sleep(0.15)

        if not all_candles:
            return pd.DataFrame()

        df = pd.DataFrame(all_candles,
                          columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'volCcy', 'volCcyQuote',
                                   'confirm'])
        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]

        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = df[col].astype(float)

        df['timestamp'] = pd.to_datetime(df['timestamp'].astype(int), unit='ms')
        if "+" in TIMEZONE:
            df['timestamp'] += pd.Timedelta(hours=int(TIMEZONE.split("+")[-1]))
        elif "-" in TIMEZONE:
            df['timestamp'] += pd.Timedelta(hours=int(TIMEZONE.split("-")[-1]))

        df.sort_values('timestamp', ascending=True, inplace=True)
        df.set_index('timestamp', inplace=True)

        current_time = self._get_current_local_time()
        if not df.empty and (current_time - df.index[-1]).total_seconds() < self._get_seconds(self.timeframe):
            df = df.iloc[:-1]

        return df

    def _fetch_historical_data_with_limit(self, limit=50000):
        """
        å†…éƒ¨æ–¹æ³•ï¼šä½¿ç”¨ç°æœ‰é€»è¾‘æ‹‰å–æŒ‡å®šæ•°é‡çš„Kçº¿
        è¿™æ˜¯åŸ fetch_historical_data çš„æ ¸å¿ƒé€»è¾‘ï¼Œä½†ä¸åŒ…å«æ—¥æœŸèŒƒå›´è¿‡æ»¤
        """
        logging.info(f"ğŸ” å‡†å¤‡åŠ è½½ {self.symbol} ({self.timeframe}) æ•°æ®...")
        local_df = self.load_local_data()

        if local_df.empty:
            logging.info(f"âš ï¸ æœ¬åœ°æ— æ•°æ®ï¼Œå°†ä» OKX å…¨é‡æ‹‰å– {limit} æ ¹...")
            final_df = self.fetch_from_okx(limit=limit)
            self.save_local_data(final_df)
            return final_df.tail(limit)

        local_count = len(local_df)
        last_local_time = local_df.index[-1]
        oldest_local_time = local_df.index[0]
        logging.info(f"ğŸ“¦ æœ¬åœ°æ•°æ®åº“å·²å‘½ä¸­ï¼ç°æœ‰ {local_count} æ ¹ K çº¿ | åŒºé—´: {oldest_local_time} -> {last_local_time}")

        current_local = self._get_current_local_time()
        bar_seconds = self._get_seconds(self.timeframe)

        # =======================================
        # æ­¥éª¤ 1: å‘å³çœ‹ï¼è¡¥é½ã€æœ€æ–°ã€‘ç¼ºå¤±çš„ K çº¿
        # =======================================
        time_diff_seconds = (current_local - last_local_time).total_seconds()
        missing_new_bars = int(time_diff_seconds / bar_seconds)

        new_df = pd.DataFrame()
        if missing_new_bars > 0:
            logging.info(f"ğŸ”„ å‡†å¤‡å¢é‡è¡¥é½çº¦ {missing_new_bars} æ ¹ æœ€æ–° K çº¿...")
            new_df = self.fetch_from_okx(limit=missing_new_bars + 10)

        if not new_df.empty:
            combined_df = pd.concat([local_df, new_df])
            combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
            combined_df = combined_df.sort_index(ascending=True)
        else:
            combined_df = local_df

        # =======================================
        # æ­¥éª¤ 2: å‘å·¦çœ‹ï¼è¡¥é½ã€æ›´è€ã€‘çš„å†å² K çº¿
        # =======================================
        current_count = len(combined_df)
        old_df = pd.DataFrame()

        if current_count < limit:
            missing_old_bars = limit - current_count
            logging.info(f"ğŸ”„ æœ¬åœ°æ•°æ®æ€»é‡ä¸è¶³ï¼Œå‡†å¤‡å‘å‰è¿½æº¯è¡¥é½ {missing_old_bars} æ ¹ å†å² K çº¿...")

            # è®¡ç®—å½“å‰åº“ä¸­æœ€è€ä¸€æ ¹ K çº¿çš„æ—¶é—´ï¼Œå¹¶é€†å‘å‰¥ç¦»æ—¶åŒºè¿˜åŸä¸º UTC æ¯«ç§’æ—¶é—´æˆ³
            oldest_local = combined_df.index[0]
            oldest_utc = oldest_local
            if "+" in TIMEZONE:
                oldest_utc -= pd.Timedelta(hours=int(TIMEZONE.split("+")[-1]))
            elif "-" in TIMEZONE:
                oldest_utc += pd.Timedelta(hours=int(TIMEZONE.split("-")[-1]))

            oldest_ts_ms = str(int(oldest_utc.tz_localize('UTC').timestamp() * 1000))

            # æºå¸¦ after_ts æ‹‰å–æ›´æ—©çš„æ•°æ®
            old_df = self.fetch_from_okx(limit=missing_old_bars + 10, after_ts=oldest_ts_ms)

        if not old_df.empty:
            combined_df = pd.concat([old_df, combined_df])
            combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
            combined_df = combined_df.sort_index(ascending=True)

        # =======================================
        # æ­¥éª¤ 3: ä¿å­˜è‡³æœ¬åœ°æ•°æ®åº“å¹¶è¿”å›
        # =======================================
        if not new_df.empty or not old_df.empty:
            self.save_local_data(combined_df)

        return combined_df.tail(limit)

    def fetch_historical_data(self, limit=50000) -> pd.DataFrame:
        """
        å…¨é‡æ™ºèƒ½æ‹¼æ¥ç³»ç»Ÿï¼š
        åˆ†ç¦»äº†ã€å¢é‡æ‹‰å–æœ€æ–°æ•°æ®ã€‘å’Œã€è¿½æº¯æ‹‰å–å†å²æ•°æ®ã€‘ä¸¤ä¸ªåŠ¨ä½œ
        """
        return self._fetch_historical_data_with_limit(limit)

    def _get_seconds(self, timeframe: str) -> int:
        mapping = {
            '1m': 60,
            '5m': 300,
            '15m': 900,
            '30m': 1800,  # <--- åŠ ä¸Š 1800 ç§’ï¼
            '1H': 3600,
            '4H': 14400,
            '1D': 86400
        }
        if timeframe not in mapping:
            raise IndexError(f"æ²¡æœ‰è¿™ä¸ªtimeframe: {timeframe}")

        return mapping.get(timeframe)

    def _calculate_bars_needed(self, start_date, end_date):
        """è®¡ç®—ä» start_date åˆ° end_date ä¹‹é—´éœ€è¦å¤šå°‘æ ¹ K çº¿"""
        if isinstance(start_date, str):
            start_date = pd.Timestamp(start_date)
        if isinstance(end_date, str):
            end_date = pd.Timestamp(end_date)

        bar_seconds = self._get_seconds(self.timeframe)
        total_seconds = (end_date - start_date).total_seconds()
        # å‘ä¸Šå–æ•´ï¼Œç¡®ä¿è¦†ç›–æ•´ä¸ªæ—¶é—´æ®µ
        bars_needed = int(total_seconds // bar_seconds) + 1
        return max(bars_needed, 0)

    def fetch_data_by_date_range(self, start_date, end_date):
        """
        æ™ºèƒ½è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
        ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ•°æ®åº“ï¼Œåªæ‹‰å–ç¼ºå¤±çš„éƒ¨åˆ†
        """
        if isinstance(start_date, str):
            start_date = pd.Timestamp(start_date)
        if isinstance(end_date, str):
            end_date = pd.Timestamp(end_date)

        # é¦–å…ˆåŠ è½½æœ¬åœ°æ•°æ®
        local_df = self.load_local_data()

        if not local_df.empty:
            # è¿‡æ»¤æœ¬åœ°æ•°æ®åœ¨æ—¶é—´æ®µå†…çš„éƒ¨åˆ†
            mask = (local_df.index >= start_date) & (local_df.index <= end_date)
            local_in_range = local_df[mask]

            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±
            if len(local_in_range) > 0:
                # è®¡ç®—æ—¶é—´æ®µå†…é¢„æœŸçš„Kçº¿æ•°é‡
                expected_bars = self._calculate_bars_needed(start_date, end_date)

                # å¦‚æœæœ¬åœ°æ•°æ®å·²ç»è¶³å¤Ÿï¼Œç›´æ¥è¿”å›
                if len(local_in_range) >= expected_bars:
                    logging.info(f"âœ… æœ¬åœ°æ•°æ®åº“å·²å®Œå…¨è¦†ç›– {start_date} åˆ° {end_date} çš„æ•°æ®ï¼Œå…± {len(local_in_range)} æ ¹ K çº¿")
                    return local_in_range

        # æœ¬åœ°æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨ç°æœ‰çš„å¢é‡é€»è¾‘æ‹‰å–æ•°æ®
        # è®¡ç®—éœ€è¦çš„æ€»Kçº¿æ•°é‡ï¼ˆç¨å¾®å¤šæ‹‰ä¸€äº›ä»¥ç¡®ä¿è¦†ç›–ï¼‰
        bars_needed = self._calculate_bars_needed(start_date, end_date)
        if bars_needed == 0:
            logging.warning(f"æ—¶é—´æ®µ {start_date} åˆ° {end_date} æ— æ•ˆæˆ–è¿‡çŸ­")
            return pd.DataFrame()

        # å¤šæ‹‰10%çš„ç¼“å†²ï¼Œç¡®ä¿å®Œå…¨è¦†ç›–
        buffer_bars = int(bars_needed * 1.1) + 10
        logging.info(f"ğŸ”„ å‡†å¤‡æ‹‰å–çº¦ {buffer_bars} æ ¹ K çº¿ä»¥è¦†ç›– {start_date} åˆ° {end_date}")

        # ä½¿ç”¨ç°æœ‰çš„å¢é‡é€»è¾‘æ‹‰å–æ•°æ®
        fetched_df = self._fetch_historical_data_with_limit(limit=buffer_bars)

        if fetched_df.empty:
            logging.error("æ‹‰å–æ•°æ®å¤±è´¥")
            return pd.DataFrame()

        # è¿‡æ»¤åˆ°æŒ‡å®šæ—¶é—´èŒƒå›´
        mask = (fetched_df.index >= start_date) & (fetched_df.index <= end_date)
        result_df = fetched_df[mask]

        if not result_df.empty:
            logging.info(f"âœ… æˆåŠŸè·å– {start_date} åˆ° {end_date} çš„æ•°æ®ï¼Œå…± {len(result_df)} æ ¹ K çº¿")
        else:
            logging.warning(f"âš ï¸ æ‹‰å–çš„æ•°æ®ä¸­æœªæ‰¾åˆ° {start_date} åˆ° {end_date} èŒƒå›´å†…çš„æ•°æ®")

        return result_df
